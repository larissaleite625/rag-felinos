{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6786e78f-0671-4429-857a-f6b1f4adfb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q pypdf python-docx python-pptx sentence-transformers faiss-cpu tqdm google-generativeai anthropic openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce0709a-a9a2-4d4e-91d3-59971f05d482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacb6df9-ff58-4824-bdf5-9a3ba0947439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os, re, json, time, uuid, math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import docx\n",
    "from pptx import Presentation\n",
    "\n",
    "# Embeddings e reranking \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# LLMs\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "import requests\n",
    "\n",
    "# Databricks secrets\n",
    "OPENAI_KEY   = dbutils.secrets.get(\"OPENAI\",   \"OPENAI_API_KEY\")\n",
    "ANTH_KEY     = dbutils.secrets.get(\"CLAUDE\",   \"ANTHROPIC_API_KEY\")\n",
    "GEMINI_KEY   = dbutils.secrets.get(\"GEMINI\",   \"GEMINI_API_KEY\")\n",
    "DEEPSEEK_KEY = dbutils.secrets.get(\"DEEPSEEK\", \"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Caminho do documento \n",
    "DOC_PATH = \"/Volumes/bronze/default/documentos_agent/Manual de Grandes Felinos CarnÃ­voros.pdf\"\n",
    "\n",
    "CATALOG = \"bronze\"\n",
    "SCHEMA  = \"default\"\n",
    "\n",
    "TABLE_DOCS_RAW   = f\"{CATALOG}.{SCHEMA}.docs_raw\"\n",
    "TABLE_CHUNKS     = f\"{CATALOG}.{SCHEMA}.docs_chunks\"\n",
    "TABLE_EMBEDS     = f\"{CATALOG}.{SCHEMA}.docs_embeddings\"\n",
    "TABLE_AUDIT      = f\"{CATALOG}.{SCHEMA}.rag_audit\"\n",
    "\n",
    "# Evitar appends (LimitaÃ§Ãµes do Free Edition): \n",
    "OVERWRITE_MODE = \"overwrite\"\n",
    "\n",
    "# ParÃ¢metros de chunking\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "\n",
    "# Modelo de embeddings (multilÃ­ngue, open, bom em PT-BR e bom pra diversas linguas)\n",
    "# embeddings sÃ£o representaÃ§Ãµes numÃ©ricas de objetos, como palavras, frases, imagens, Ã¡udios ou atÃ© mesmo usuÃ¡rios e itens. Eles transformam esses dados complexos em vetores densos de nÃºmeros.\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "\n",
    "# Top-k padrÃ£o\n",
    "TOP_K_DEFAULT = 6\n",
    "# ParÃ¢metro que determina quantos chunks o sistema de busca deve encontrar e enviar para o LLM gerar a resposta.\n",
    "\n",
    "\n",
    "\n",
    "# Seed FAISS\n",
    "np.random.seed(42)\n",
    "\n",
    "#Ã­ndice vetorial da mesma maneira todas as vezes,\n",
    "#Ele forÃ§a o FAISS a construir o seu Ã­ndice vetorial da mesma maneira todas as vezes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd368bc-cbcd-4109-a406-cd9c12526114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_pdf(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    texts = []\n",
    "    for page in reader.pages:\n",
    "        try:\n",
    "            texts.append(page.extract_text() or \"\")\n",
    "        except Exception:\n",
    "            texts.append(\"\")\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def read_docx(path: str) -> str:\n",
    "    d = docx.Document(path)\n",
    "    paras = []\n",
    "    for p in d.paragraphs:\n",
    "        paras.append(p.text)\n",
    "    return \"\\n\".join(paras)\n",
    "\n",
    "def read_pptx(path: str) -> str:\n",
    "    prs = Presentation(path)\n",
    "    slides_txt = []\n",
    "    for i, slide in enumerate(prs.slides):\n",
    "        buf = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                buf.append(shape.text)\n",
    "        slides_txt.append(f\"[SLIDE {i+1}]\\n\" + \"\\n\".join(buf))\n",
    "    return \"\\n\\n\".join(slides_txt)\n",
    "\n",
    "def load_text_from_path(path: str) -> Tuple[str, str]:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return read_pdf(path), \"pdf\"\n",
    "    elif ext in [\".docx\"]:\n",
    "        return read_docx(path), \"docx\"\n",
    "    elif ext in [\".pptx\"]:\n",
    "        return read_pptx(path), \"pptx\"\n",
    "    else:\n",
    "        raise ValueError(f\"ExtensÃ£o nÃ£o suportada: {ext} (suporte: .pdf, .docx, .pptx)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d292bf69-192f-4d17-9e98-c0be9692c8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    # Quebra simples por parÃ¡grafos + fallback por caracteres\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n{2,}\", text) if p.strip()]\n",
    "    if not paragraphs:\n",
    "        paragraphs = [text]\n",
    "\n",
    "    chunks = []\n",
    "    buf = \"\"\n",
    "    for p in paragraphs:\n",
    "        if len(buf) + len(p) + 1 <= chunk_size:\n",
    "            buf = (buf + \"\\n\" + p).strip()\n",
    "        else:\n",
    "            # fecha o chunk atual\n",
    "            if buf:\n",
    "                chunks.append(buf)\n",
    "            # inicia novo acumulador, com overlap do final do anterior\n",
    "            if chunks and overlap > 0:\n",
    "                tail = buf[-overlap:]\n",
    "                buf = (tail + \"\\n\" + p).strip()\n",
    "            else:\n",
    "                buf = p\n",
    "        # flush final\n",
    "    if buf:\n",
    "        chunks.append(buf)\n",
    "\n",
    "    # Garantir que nenhum chunk extrapole chunk_size\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= chunk_size:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            # fatiar por tamanho duro\n",
    "            start = 0\n",
    "            while start < len(c):\n",
    "                end = min(start + chunk_size, len(c))\n",
    "                final.append(c[start:end])\n",
    "                start = end - overlap if overlap > 0 else end\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb07a3e9-5fa5-49ed-a004-c067be3102f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_text, filetype = load_text_from_path(DOC_PATH)\n",
    "\n",
    "raw_df = spark.createDataFrame(\n",
    "    [(str(uuid.uuid4()), os.path.basename(DOC_PATH), DOC_PATH, filetype, raw_text, datetime.utcnow())],\n",
    "    schema=T.StructType([\n",
    "        T.StructField(\"doc_id\", T.StringType(), False),\n",
    "        T.StructField(\"filename\", T.StringType(), False),\n",
    "        T.StructField(\"path\", T.StringType(), False),\n",
    "        T.StructField(\"filetype\", T.StringType(), False),\n",
    "        T.StructField(\"text\", T.StringType(), False),\n",
    "        T.StructField(\"ingested_at_utc\", T.TimestampType(), False),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# CREATE IF NOT EXISTS\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_DOCS_RAW} (\n",
    "  doc_id STRING,\n",
    "  filename STRING,\n",
    "  path STRING,\n",
    "  filetype STRING,\n",
    "  text STRING,\n",
    "  ingested_at_utc TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "#lÃª existente, faz uniÃ£o e OVERWRITE\n",
    "try:\n",
    "    existing = spark.table(TABLE_DOCS_RAW)\n",
    "    union_df = existing.unionByName(raw_df, allowMissingColumns=True)\n",
    "except Exception:\n",
    "    union_df = raw_df\n",
    "\n",
    "union_df.write.mode(OVERWRITE_MODE).format(\"delta\").saveAsTable(TABLE_DOCS_RAW)\n",
    "\n",
    "display(spark.table(TABLE_DOCS_RAW).limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d934e3-42c5-4af9-9b0c-4ad26366960f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T, Window\n",
    "\n",
    "# ParÃ¢metros\n",
    "CHUNK_SIZE = 800\n",
    "#FragmentaÃ§Ã£o da informaÃ§Ã£o\n",
    "\n",
    "\n",
    "CHUNK_OVERLAP = 120\n",
    "#CHUNK_OVERLAP Ã© a quantidade de caracteres que um chunk deve compartilhar com o prÃ³ximo. Dos 800, 120 sÃ£o comuns entre dois chunks. \n",
    "# O overlap Ã© crucial para nÃ£o perder contexto nas bordas dos chunks.\n",
    "\n",
    "\n",
    "# Para teste: sÃ³ o documento atual e textos nÃ£o nulos\n",
    "raw_df = (\n",
    "    spark.table(TABLE_DOCS_RAW)\n",
    "         .filter(F.col(\"path\") == DOC_PATH)\n",
    "         .filter(F.col(\"text\").isNotNull() & (F.length(\"text\") > 0))\n",
    "         .select(\"doc_id\", \"text\")\n",
    ")\n",
    "\n",
    "# Passo (janela deslizante com overlap). Garante que step >= 1\n",
    "step = F.greatest(F.lit(1), F.lit(CHUNK_SIZE - CHUNK_OVERLAP))\n",
    "# Exemplo: Com 800 - 120 = 680. Cada novo chunk comeÃ§arÃ¡ 680 caracteres depois do anterior, garantindo uma sobreposiÃ§Ã£o de 120 caracteres.\n",
    "\n",
    "\n",
    "# InÃ­cio de cada janela: 0, step, 2*step, ... atÃ© o Ãºltimo inÃ­cio possÃ­vel\n",
    "max_start = F.greatest(F.length(\"text\") - F.lit(CHUNK_SIZE), F.lit(0))\n",
    "starts = F.sequence(F.lit(0), max_start, step)\n",
    "\n",
    "# Explode as janelas e gera os chunks com substring (1-indexed em Spark)\n",
    "chunks_df = (\n",
    "    raw_df\n",
    "    .withColumn(\"start\", F.explode(starts))\n",
    "    .withColumn(\"chunk_text\", F.substring(F.col(\"text\"), F.col(\"start\") + F.lit(1), F.lit(CHUNK_SIZE)))\n",
    "    .withColumn(\"chunk_text\", F.trim(F.col(\"chunk_text\")))\n",
    "    .filter(F.length(\"chunk_text\") > 0)\n",
    ")\n",
    "\n",
    "# Atribui chunk_id sequencial por doc\n",
    "w = Window.partitionBy(\"doc_id\").orderBy(\"start\")\n",
    "chunks_df = (\n",
    "    chunks_df\n",
    "    .withColumn(\"chunk_id\", F.row_number().over(w) - 1)\n",
    "    .select(\"doc_id\", \"chunk_id\", \"chunk_text\")\n",
    ")\n",
    "\n",
    "# Cria a tabela se nÃ£o existir\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_CHUNKS} (\n",
    "  doc_id STRING,\n",
    "  chunk_id INT,\n",
    "  chunk_text STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# OVERWRITE (sem union, sem coletar no driver)\n",
    "(chunks_df\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(TABLE_CHUNKS)\n",
    ")\n",
    "\n",
    "display(spark.table(TABLE_CHUNKS).limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d33c417-9b05-4a1b-9e32-96aeb4955ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carrega todos os chunks em memÃ³ria para embed\n",
    "chunks_pd = spark.table(TABLE_CHUNKS).orderBy(\"doc_id\",\"chunk_id\").toPandas()\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "embeds = embed_model.encode(chunks_pd[\"chunk_text\"].tolist(), batch_size=64, normalize_embeddings=True)\n",
    "\n",
    "# Prepara DF spark com vetor como ARRAY<DOUBLE>\n",
    "def to_list_float32(v: np.ndarray) -> List[float]:\n",
    "    return [float(x) for x in v.astype(np.float32).tolist()]\n",
    "\n",
    "embedded_rows = []\n",
    "for (doc_id, chunk_id, chunk_text), vec in zip(chunks_pd[[\"doc_id\",\"chunk_id\",\"chunk_text\"]].values, embeds):\n",
    "    embedded_rows.append( (doc_id, int(chunk_id), to_list_float32(vec)) )\n",
    "\n",
    "emb_schema = T.StructType([\n",
    "    T.StructField(\"doc_id\", T.StringType(), False),\n",
    "    T.StructField(\"chunk_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"embedding\", T.ArrayType(T.FloatType()), False),\n",
    "])\n",
    "\n",
    "emb_df = spark.createDataFrame(embedded_rows, emb_schema)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_EMBEDS} (\n",
    "  doc_id STRING,\n",
    "  chunk_id INT,\n",
    "  embedding ARRAY<FLOAT>\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    existing = spark.table(TABLE_EMBEDS)\n",
    "    union_df = existing.unionByName(emb_df, allowMissingColumns=True)\n",
    "except Exception:\n",
    "    union_df = emb_df\n",
    "\n",
    "union_df.write.mode(OVERWRITE_MODE).format(\"delta\").saveAsTable(TABLE_EMBEDS)\n",
    "\n",
    "display(spark.table(TABLE_EMBEDS).limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e2efa1-29a0-4131-a26c-aadf201da939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Etapa de IndexaÃ§Ã£o do RAG\n",
    "# Carrega embeddings e monta FAISS\n",
    "emb_pd = spark.table(TABLE_EMBEDS).orderBy(\"doc_id\",\"chunk_id\").toPandas()\n",
    "\n",
    "\n",
    "mat = np.vstack(emb_pd[\"embedding\"].apply(lambda v: np.array(v, dtype=\"float32\")).to_list())\n",
    "index = faiss.IndexFlatIP(mat.shape[1])   \n",
    "# Similaridade por produto interno (usa embeddings normalizados)\n",
    "# IndexFlatIP: Este Ã© o tipo de Ã­ndice mais simples (mais lento em grandes escalas, mas muito preciso) que usa Produto Interno (IP - Inner Product) para medir a similaridade. O Produto Interno Ã© usado quando os embeddings foram previamente normalizados (o que Ã© padrÃ£o em muitos modelos), sendo equivalente Ã  similaridade de cosseno.\n",
    "index.add(mat)\n",
    "# AdiÃ§Ã£o dos Vetores: index.add(mat): Insere todos os embeddings da matriz (mat) no Ã­ndice FAISS.\n",
    "\n",
    "# Mapa (linha -> (doc_id, chunk_id))\n",
    "line2meta = list(zip(emb_pd[\"doc_id\"].tolist(), emb_pd[\"chunk_id\"].tolist()))\n",
    "\n",
    "# Para recuperar o texto do chunk\n",
    "chunks_map = {\n",
    "    (r[\"doc_id\"], int(r[\"chunk_id\"])): r[\"chunk_text\"]\n",
    "    for r in spark.table(TABLE_CHUNKS).collect()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf78391-0cd1-4795-b87f-083f80268ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\"OPENAI\", \"OPENAI_API_KEY\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = dbutils.secrets.get(\"CLAUDE\", \"ANTHROPIC_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"]     = dbutils.secrets.get(\"GEMINI\", \"GEMINI_API_KEY\")\n",
    "os.environ[\"DEEPSEEK_API_KEY\"]   = dbutils.secrets.get(\"DEEPSEEK\", \"DEEPSEEK_API_KEY\")\n",
    "\n",
    "def call_openai(prompt: str, model=\"gpt-4o-mini\", temperature=0.0) -> str:\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":\"Responda em portuguÃªs do Brasil e cite apenas o contexto fornecido.\"},\n",
    "            {\"role\":\"user\",\"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "def call_anthropic(prompt: str, model=\"claude-3-haiku-20240307\", temperature=0.0) -> str:\n",
    "    client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "    resp = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1500,\n",
    "        temperature=temperature,\n",
    "        system=\"Responda em portuguÃªs do Brasil e cite apenas o contexto fornecido.\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return \"\".join([b.text for b in resp.content if b.type==\"text\"])\n",
    "\n",
    "\n",
    "def call_gemini(prompt: str, model=\"gemini-1.5-flash\", temperature=0.0) -> str:\n",
    "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    g = genai.GenerativeModel(model)\n",
    "\n",
    "    resp = g.generate_content(\n",
    "        [\n",
    "            {\"text\": \"Responda em portuguÃªs do Brasil e cite apenas o contexto fornecido.\"},\n",
    "            {\"text\": prompt}\n",
    "        ],\n",
    "        generation_config={\"temperature\": temperature}\n",
    "    )\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def call_deepseek(prompt: str, model=\"deepseek-chat\", temperature=0.0) -> str:\n",
    "    api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\":\"system\",\"content\":\"Responda em portuguÃªs do Brasil e cite apenas o contexto fornecido.\"},\n",
    "            {\"role\":\"user\",\"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    r = requests.post(\"https://api.deepseek.com/chat/completions\", headers=headers, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def call_llm(provider: str, prompt: str, model: str=None, temperature: float=0.0) -> str:\n",
    "    p = provider.strip().lower()\n",
    "    if p == \"openai\":\n",
    "        return call_openai(prompt, model or \"gpt-4o-mini\", temperature)\n",
    "    if p == \"anthropic\":\n",
    "        return call_anthropic(prompt, model or \"claude-3-haiku-20240307\", temperature)\n",
    "    if p == \"gemini\":\n",
    "        return call_gemini(prompt, model or \"gemini-1.5-flash\", temperature)\n",
    "    if p == \"deepseek\":\n",
    "        return call_deepseek(prompt, model or \"deepseek-chat\", temperature)\n",
    "    raise ValueError(\"Provider invÃ¡lido. Use: openai | anthropic | gemini | deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc33c9aa-6373-4f3e-901d-a27477775915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def embed_query(q: str) -> np.ndarray:\n",
    "    v = embed_model.encode([q], normalize_embeddings=True)[0]\n",
    "    return v.astype(\"float32\")\n",
    "\n",
    "def retrieve(q: str, k: int=TOP_K_DEFAULT) -> List[Tuple[str, int, str]]:\n",
    "    v = embed_query(q)\n",
    "    D, I = index.search(v.reshape(1, -1), k)\n",
    "    hits = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0: \n",
    "            continue\n",
    "        doc_id, chunk_id = line2meta[idx]\n",
    "        hits.append( (doc_id, int(chunk_id), chunks_map[(doc_id, int(chunk_id))]) )\n",
    "    return hits\n",
    "\n",
    "def build_prompt(question: str, passages: List[Tuple[str,int,str]]) -> str:\n",
    "    ctx_parts = []\n",
    "    for i,(doc_id, chunk_id, text) in enumerate(passages):\n",
    "        ctx_parts.append(f\"[{i}] doc_id={doc_id} chunk_id={chunk_id}\\n{text}\")\n",
    "    ctx = \"\\n\\n\".join(ctx_parts)\n",
    "    prompt = f\"\"\"VocÃª Ã© um assistente RAG. Responda **APENAS** com base nos trechos abaixo.\n",
    "Se faltar evidÃªncia, diga explicitamente que nÃ£o sabe.\n",
    "Inclua referÃªncias entre colchetes usando os Ã­ndices [i] mostrados nos trechos relevantes.\n",
    "\n",
    "<<<TRECHOS>>>\n",
    "{ctx}\n",
    "<<<FIM TRECHOS>>>\n",
    "\n",
    "Pergunta: {question}\n",
    "Resposta:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29be4e2d-9cd4-4f15-b483-807031e14919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tabela de auditoria\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_AUDIT} (\n",
    "  run_id STRING,\n",
    "  ts_utc TIMESTAMP,\n",
    "  provider STRING,\n",
    "  model STRING,\n",
    "  question STRING,\n",
    "  top_k INT,\n",
    "  doc_ids ARRAY<STRING>,\n",
    "  chunk_ids ARRAY<INT>,\n",
    "  latency_ms DOUBLE,\n",
    "  response_preview STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "def audit_log(provider: str, model: str, question: str, top_k: int,\n",
    "              passages: List[Tuple[str,int,str]], latency_ms: float, response_preview: str):\n",
    "    row = [(str(uuid.uuid4()),\n",
    "            datetime.utcnow(),\n",
    "            provider, model or \"\", question, int(top_k),\n",
    "            [p[0] for p in passages],\n",
    "            [p[1] for p in passages],\n",
    "            float(latency_ms),\n",
    "            response_preview[:1000])]\n",
    "    df = spark.createDataFrame(row, schema=T.StructType([\n",
    "        T.StructField(\"run_id\", T.StringType(), False),\n",
    "        T.StructField(\"ts_utc\", T.TimestampType(), False),\n",
    "        T.StructField(\"provider\", T.StringType(), False),\n",
    "        T.StructField(\"model\", T.StringType(), False),\n",
    "        T.StructField(\"question\", T.StringType(), False),\n",
    "        T.StructField(\"top_k\", T.IntegerType(), False),\n",
    "        T.StructField(\"doc_ids\", T.ArrayType(T.StringType()), False),\n",
    "        T.StructField(\"chunk_ids\", T.ArrayType(T.IntegerType()), False),\n",
    "        T.StructField(\"latency_ms\", T.DoubleType(), False),\n",
    "        T.StructField(\"response_preview\", T.StringType(), False),\n",
    "    ]))\n",
    "    # Sem append: unir com existente e OVERWRITE\n",
    "    try:\n",
    "        existing = spark.table(TABLE_AUDIT)\n",
    "        out = existing.unionByName(df, allowMissingColumns=True)\n",
    "    except Exception:\n",
    "        out = df\n",
    "    out.write.mode(OVERWRITE_MODE).format(\"delta\").saveAsTable(TABLE_AUDIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821fa71c-0403-4a3b-9349-da48c60129f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from openai import OpenAI\n",
    "\n",
    "# ask() com datetime correto e logs na auditoria\n",
    "def ask(\n",
    "    question: str,\n",
    "    provider: str=\"openai\",\n",
    "    model: str=None,\n",
    "    k: int=TOP_K_DEFAULT,\n",
    "    temperature: float=0.0\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Recupera chunks relevantes\n",
    "    passages = retrieve(question, k=k)\n",
    "    prompt = build_prompt(question, passages)\n",
    "\n",
    "    # Chama o LLM escolhido\n",
    "    answer = call_llm(provider, prompt, model=model, temperature=temperature)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Auditoria\n",
    "    audit_log(\n",
    "        provider,\n",
    "        model or \"\",\n",
    "        question,\n",
    "        k,\n",
    "        passages,\n",
    "        (t1 - t0)*1000.0,\n",
    "        answer\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"provider\": provider,\n",
    "        \"model\": model,\n",
    "        \"question\": question,\n",
    "        \"top_k\": k,\n",
    "        \"passages_used\": [\n",
    "            {\"i\": i, \"doc_id\": d, \"chunk_id\": c}\n",
    "            for i,(d,c,_) in enumerate(passages)\n",
    "        ],\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "# ConversaÃ§Ã£o interativa\n",
    "print(\"ðŸ¤– Agente RAG ativado! Para sair, digite: sair\")\n",
    "\n",
    "provider = input(\"Escolha o provedor (openai/anthropic/gemini/deepseek): \").strip().lower()\n",
    "if provider not in [\"openai\",\"anthropic\",\"gemini\",\"deepseek\"]:\n",
    "    print(\"Provedor invÃ¡lido, usando openai.\")\n",
    "    provider = \"openai\"\n",
    "\n",
    "model = input(\"Modelo (pressione Enter para padrÃ£o): \").strip() or None\n",
    "\n",
    "try:\n",
    "    temperature = float(input(\"Temperatura (0â€“1, padrÃ£o 0): \") or 0)\n",
    "except:\n",
    "    temperature = 0\n",
    "\n",
    "print(f\"\\nâœ… Provider: {provider}\")\n",
    "print(f\"âœ… Modelo: {model or 'padrÃ£o'}\")\n",
    "print(f\"âœ… Temperatura: {temperature}\")\n",
    "print(\"âœ… RAG carregado\")\n",
    "print(\"\\nComece a conversar!\\n\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"VocÃª: \")\n",
    "    if question.lower() in [\"sair\",\"exit\",\"quit\"]:\n",
    "        print(\"ðŸ‘‹ Encerrando conversa\")\n",
    "        break\n",
    "    \n",
    "    res = ask(question, provider=provider, model=model, temperature=temperature)\n",
    "    print(f\"\\nðŸ¤– IA ({provider}):\\n{res['answer']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rag-teste",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
